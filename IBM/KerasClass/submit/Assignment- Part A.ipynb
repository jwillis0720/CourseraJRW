{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:14:41.241432Z",
     "start_time": "2019-11-17T09:14:38.505815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A -- \n",
    "\n",
    "A. Build a baseline model (5 marks)\n",
    "\n",
    "Use the Keras library to build a neural network with the following:\n",
    "\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split helper function from Scikit-learn.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:15:02.574995Z",
     "start_time": "2019-11-17T09:15:02.262446Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import Concrete data as a pandas dataframe\n",
    "concrete_data = pandas.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:15:03.672416Z",
     "start_time": "2019-11-17T09:15:03.657644Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(all_history_loss_train,all_history_loss_test):\n",
    "    \n",
    "    '''This is a plotting function to visualize the loss using seaborn\n",
    "    \n",
    "    pip install seaborn\n",
    "    \n",
    "    '''\n",
    "    plotting_list = []\n",
    "    for i in range(len(all_history_loss_train)):\n",
    "        for epoch in range(1,len(all_history_loss_train[i])+1):\n",
    "            plotting_list.append(\n",
    "                {\n",
    "                    'repeat':i+1,\n",
    "                    'epoch':epoch,\n",
    "                    'loss':all_history_loss_train[i][epoch-1],\n",
    "                    'loss_type':'Train'\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "        for epoch in range(1,len(all_history_loss_test[i])+1):\n",
    "            plotting_list.append(\n",
    "                {\n",
    "                    'repeat':i+1,\n",
    "                    'epoch':epoch,\n",
    "                    'loss':all_history_loss_test[i][epoch-1],\n",
    "                    'loss_type':'Test'\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "\n",
    "    pd = pandas.DataFrame(plotting_list)\n",
    "    sns.lineplot(x='epoch',y='loss',data=pd,hue='loss_type')\n",
    "\n",
    "\n",
    "def get_norm_of_dataframe(df):\n",
    "    '''normalize a dataframe and return it'''\n",
    "    return (df - df.mean())/df.std()\n",
    "\n",
    "\n",
    "def get_mean_square_are_of_model(epochs,hidden_layers=1,normal=False):\n",
    "    ##Get a random %30 of pandas dataframe with the sample method\n",
    "    test_data = concrete_data.sample(int(len(concrete_data)*.3))\n",
    "\n",
    "\n",
    "    ##Train data is just concrete data not found in test_data\n",
    "    train_data = concrete_data[~concrete_data.index.isin(test_data.index)]\n",
    "\n",
    "    ##Slice the dataframe so the input is everything except strength column\n",
    "    x_test = test_data[[c for c in test_data.columns if c !='Strength']]\n",
    "    x_train = train_data[[c for c in train_data.columns if c !='Strength']]\n",
    "\n",
    "    ##The output data will be the strength data\n",
    "    y_test = test_data['Strength']\n",
    "    y_train = train_data['Strength']\n",
    "    \n",
    "    \n",
    "    if normal:\n",
    "        ##Normalize dataset\n",
    "        x_test_norm, x_train_norm = get_norm_of_dataframe(x_test), get_norm_of_dataframe(x_train)\n",
    "    else:\n",
    "        x_test_norm, x_train_norm = x_test, x_train\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    n_cols = len(x_train.columns)\n",
    "    ###Add hidden with extrapolated input from n_cols\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    if hidden_layers > 1:\n",
    "        for x in range(1,hidden_layers):\n",
    "            model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    #plot model\n",
    "    plot_model(model,show_layer_names=False,show_shapes=True)\n",
    "    \n",
    "    '''\n",
    "    Fit the model on the training data\n",
    "    \n",
    "    While the assignment says use validation on an evaluation step, you can also just do it in place. It helps in plotting\n",
    "    '''\n",
    "    history = model.fit(x_train_norm, y_train, validation_data=(x_test_norm,y_test),epochs=epochs,batch_size=10,verbose=0)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    As the assignment says, I will also use the predict function on the testing data to get predicted output\n",
    "    from the other 30% data\n",
    "    \n",
    "    '''\n",
    "    y_predicted = model.predict(x_test_norm)\n",
    "    \n",
    "    ##Now you can take the y_prediction and y_test and get their mean_square_error using the sklern function\n",
    "    mse = mean_squared_error(y_test,y_predicted)\n",
    "\n",
    "    #I want to return mse of the y_test - y_predicted as the assignment specifies.\n",
    "    #I also want to return history so I can plot\n",
    "    return history, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:18:36.217726Z",
     "start_time": "2019-11-17T09:15:18.693220Z"
    }
   },
   "outputs": [],
   "source": [
    "##Make a list to append all mean squared errors from the test datasets\n",
    "all_errors = []\n",
    "all_history_loss_test = []\n",
    "all_history_loss_train = []\n",
    "\n",
    "##Repeat 50 times\n",
    "for repeats in range(50):\n",
    "    #returns and history and mean square error\n",
    "    \n",
    "    '''\n",
    "    In part A we don't want data normalized, 50 epochs and 1 hidden layer\n",
    "    '''\n",
    "    h,mse = get_mean_square_are_of_model(50,hidden_layers=1, normal=False)\n",
    "    ##append error\n",
    "    all_errors.append(mse)\n",
    "    \n",
    "    ##Append the loss and the test value loss\n",
    "    all_history_loss_train.append(h.history['loss'])\n",
    "    all_history_loss_test.append(h.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:18:36.224341Z",
     "start_time": "2019-11-17T09:18:36.220398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error - 127.28\n",
      "Standard Deviation of Error - 30.99\n"
     ]
    }
   ],
   "source": [
    "#convert errors to numpy array\n",
    "errors = np.array(all_errors)\n",
    "\n",
    "#Print it\n",
    "print('Mean Error - {:.5}\\nStandard Deviation of Error - {:.4}'.format(errors.mean(), errors.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T09:18:51.927952Z",
     "start_time": "2019-11-17T09:18:50.803131Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Part A\\nMean Error - {:.5}\\nStandard Deviation of Error - {:.4}'.format(errors.mean(), errors.std()))\n",
    "plot_loss(all_history_loss_test, all_history_loss_train)\n",
    "plt.savefig('PartA.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Part A\n",
    " ![PartA](https://raw.githubusercontent.com/jwillis0720/CourseraJRW/master/PartA.png#left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
